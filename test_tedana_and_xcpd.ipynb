{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRIPrep + tedana + XCPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Run fMRIPrep 22.0.0+ with `--me-output-echos` flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Remove dummy scans from fMRIPrep files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tedana.workflows import tedana_workflow\n",
    "\n",
    "\n",
    "def _flag_dummyvols(confounds_file):\n",
    "    confounds_df = pd.read_table(confounds_file)\n",
    "    nss_cols = [c for c in confounds_df.columns if c.startswith(\"non_steady_state_outlier\")]\n",
    "    if nss_cols:\n",
    "        initial_volumes_df = confounds_df[nss_cols]\n",
    "        dummy_vols = np.any(initial_volumes_df.to_numpy(), axis=1)\n",
    "        dummy_vols = np.where(dummy_vols)[0]\n",
    "\n",
    "        # reasonably assumes all NSS volumes are contiguous\n",
    "        n_dummy_vols = int(dummy_vols[-1] + 1)\n",
    "        # dummy_scans = 10\n",
    "    else:\n",
    "        n_dummy_vols = 0\n",
    "    \n",
    "    return n_dummy_vols\n",
    "\n",
    "\n",
    "def _remove_dummyvols(in_file, out_file, n_dummy_vols):\n",
    "    \"\"\"Prepare data.\"\"\"\n",
    "    if n_dummy_vols:\n",
    "        print(f\"Dropping {n_dummy_vols} volumes from {os.path.basename(in_file)}\")\n",
    "\n",
    "        img = nib.load(in_file)\n",
    "        img = img.slicer[..., n_dummy_vols:]\n",
    "        img.to_filename(out_file)\n",
    "    else:\n",
    "        out_file = in_file\n",
    "\n",
    "    return out_file\n",
    "\n",
    "\n",
    "def drop_dummy_vols(bold_files, confounds_file, temp_dir=\".\"):\n",
    "    shortened_files = []\n",
    "    n_dummy_vols = _flag_dummyvols(confounds_file)\n",
    "    for bold_file in bold_files:\n",
    "        if n_dummy_vols:\n",
    "            temp_file = os.path.join(temp_dir, os.path.basename(bold_file))\n",
    "            shortened_file = _remove_dummyvols(bold_file, temp_file, n_dummy_vols)\n",
    "        else:\n",
    "            shortened_file = bold_file\n",
    "        shortened_files.append(shortened_file)\n",
    "\n",
    "    return shortened_files, n_dummy_vols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 4 volumes from sub-EN100_task-lppEN_run-1_echo-1_desc-preproc_bold.nii.gz\n",
      "Dropping 4 volumes from sub-EN100_task-lppEN_run-1_echo-2_desc-preproc_bold.nii.gz\n",
      "Dropping 4 volumes from sub-EN100_task-lppEN_run-1_echo-3_desc-preproc_bold.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     tedana:tedana_workflow:466 Using output directory: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1\n",
      "INFO     tedana:tedana_workflow:479 Loading input data: ['/Users/taylor/Documents/datasets/ds003643/derivatives/reduced_files/sub-EN100_task-lppEN_run-1_echo-1_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/ds003643/derivatives/reduced_files/sub-EN100_task-lppEN_run-1_echo-2_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/ds003643/derivatives/reduced_files/sub-EN100_task-lppEN_run-1_echo-3_desc-preproc_bold.nii.gz']\n",
      "INFO     tedana:tedana_workflow:561 Using user-defined mask\n",
      "INFO     tedana:tedana_workflow:609 Computing T2* map\n",
      "INFO     combine:make_optcom:242 Optimally combining data with voxel-wise T2* estimates\n",
      "INFO     tedana:tedana_workflow:634 Writing optimally combined data set: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcom_bold.nii.gz\n",
      "INFO     pca:tedpca:228 Computing PCA of optimally combined multi-echo data with selection criteria: aic\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:640: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "INFO     collect:generate_metrics:123 Calculating weight maps\n",
      "INFO     collect:generate_metrics:132 Calculating parameter estimate maps for optimally combined data\n",
      "INFO     collect:generate_metrics:145 Calculating z-statistic maps\n",
      "INFO     collect:generate_metrics:155 Calculating F-statistic maps\n",
      "INFO     collect:generate_metrics:165 Thresholding z-statistic maps\n",
      "INFO     collect:generate_metrics:172 Calculating T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:179 Calculating S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:187 Counting significant voxels in T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:193 Counting significant voxels in S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:200 Thresholding optimal combination beta maps to match T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:206 Thresholding optimal combination beta maps to match S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:213 Calculating kappa and rho\n",
      "INFO     collect:generate_metrics:222 Calculating variance explained\n",
      "INFO     collect:generate_metrics:228 Calculating normalized variance explained\n",
      "INFO     collect:generate_metrics:235 Calculating DSI between thresholded T2* F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:246 Calculating DSI between thresholded S0 F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:257 Calculating signal-noise t-statistics\n",
      "INFO     collect:generate_metrics:295 Counting significant noise voxels from z-statistic maps\n",
      "INFO     collect:generate_metrics:306 Calculating decision table score\n",
      "INFO     pca:tedpca:315 Selected 78 components with aic dimensionality detection\n",
      "INFO     ica:tedica:83 ICA with random seed 42 converged in 113 iterations\n",
      "INFO     tedana:tedana_workflow:671 Making second component selection guess from ICA results\n",
      "INFO     collect:generate_metrics:123 Calculating weight maps\n",
      "INFO     collect:generate_metrics:132 Calculating parameter estimate maps for optimally combined data\n",
      "INFO     collect:generate_metrics:145 Calculating z-statistic maps\n",
      "INFO     collect:generate_metrics:155 Calculating F-statistic maps\n",
      "INFO     collect:generate_metrics:165 Thresholding z-statistic maps\n",
      "INFO     collect:generate_metrics:172 Calculating T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:179 Calculating S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:187 Counting significant voxels in T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:193 Counting significant voxels in S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:200 Thresholding optimal combination beta maps to match T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:206 Thresholding optimal combination beta maps to match S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:213 Calculating kappa and rho\n",
      "INFO     collect:generate_metrics:222 Calculating variance explained\n",
      "INFO     collect:generate_metrics:228 Calculating normalized variance explained\n",
      "INFO     collect:generate_metrics:235 Calculating DSI between thresholded T2* F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:246 Calculating DSI between thresholded S0 F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:257 Calculating signal-noise t-statistics\n",
      "INFO     collect:generate_metrics:295 Counting significant noise voxels from z-statistic maps\n",
      "INFO     collect:generate_metrics:306 Calculating decision table score\n",
      "INFO     tedica:kundu_selection_v2:138 Performing ICA component selection with Kundu decision tree v2.5\n",
      "INFO     io:denoise_ts:374 Variance explained by decomposition: 95.75%\n",
      "INFO     io:write_split_ts:432 Writing high-Kappa time series: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcomAccepted_bold.nii.gz\n",
      "INFO     io:write_split_ts:439 Writing low-Kappa time series: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcomRejected_bold.nii.gz\n",
      "INFO     io:write_split_ts:446 Writing denoised time series: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcomDenoised_bold.nii.gz\n",
      "INFO     io:writeresults:498 Writing full ICA coefficient feature set: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-ICA_components.nii.gz\n",
      "INFO     io:writeresults:502 Writing denoised ICA coefficient feature set: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-ICAAccepted_components.nii.gz\n",
      "INFO     io:writeresults:508 Writing Z-normalized spatial component maps: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-ICAAccepted_stat-z_components.nii.gz\n",
      "INFO     tedana:tedana_workflow:889 Making figures folder with static component maps and timecourse plots.\n",
      "INFO     io:denoise_ts:374 Variance explained by decomposition: 95.75%\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:640: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "INFO     tedana:tedana_workflow:918 Generating dynamic report\n",
      "INFO     tedana:tedana_workflow:921 Workflow completed\n"
     ]
    }
   ],
   "source": [
    "dset_dir = \"/Users/taylor/Documents/datasets/ds003643/\"\n",
    "deriv_dir = os.path.join(dset_dir, \"derivatives\")\n",
    "func_dir = os.path.join(deriv_dir, \"fmriprep/sub-EN100/func\")\n",
    "\n",
    "ECHO_TIMES = [12.8, 27.5, 43]  # hardcoded bc i'm lazy\n",
    "run_number = 1  # keeping as variable bc there are 10 runs. easy to loop over\n",
    "prefix = f\"sub-EN100_task-lppEN_run-{run_number}\"\n",
    "bold_files = [\n",
    "    f\"{prefix}_echo-{echo}_desc-preproc_bold.nii.gz\" \n",
    "    for echo in range(1, len(ECHO_TIMES) + 1)\n",
    "]\n",
    "bold_files = [os.path.join(func_dir, f) for f in bold_files]\n",
    "confounds_file = os.path.join(func_dir,  f\"{prefix}_desc-confounds_timeseries.tsv\")\n",
    "mask_file = os.path.join(func_dir, f\"{prefix}_desc-brain_mask.nii.gz\")\n",
    "tedana_out_dir = os.path.join(deriv_dir, \"tedana/sub-EN100/func\", prefix)\n",
    "os.makedirs(tedana_out_dir, exist_ok=True)\n",
    "tedana_temp_dir = os.path.join(dset_dir, \"derivatives\", \"reduced_files\")\n",
    "os.makedirs(tedana_temp_dir, exist_ok=True)\n",
    "\n",
    "shortened_files, n_dummy_vols = drop_dummy_vols(\n",
    "    bold_files=bold_files,\n",
    "    confounds_file=confounds_file,\n",
    "    temp_dir=tedana_temp_dir,\n",
    ")\n",
    "tedana_workflow(\n",
    "    data=shortened_files,\n",
    "    tes=ECHO_TIMES,\n",
    "    out_dir=tedana_out_dir,\n",
    "    mask=mask_file,\n",
    "    prefix=prefix,\n",
    "    fittype=\"curvefit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Label tedana components and fill in dummy volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mixing matrix and classifications from tedana\n",
    "mixing_matrix = os.path.join(tedana_out_dir, f\"{prefix}_desc-ICA_mixing.tsv\")\n",
    "metrics_df = os.path.join(tedana_out_dir, f\"{prefix}_desc-tedana_metrics.tsv\")\n",
    "mixing_matrix = pd.read_table(mixing_matrix)\n",
    "metrics_df = pd.read_table(metrics_df)\n",
    "\n",
    "# Prepend \"signal__\" to all accepted components' column names\n",
    "accepted_columns = metrics_df.loc[metrics_df[\"classification\"] != \"rejected\", \"Component\"]\n",
    "mixing_matrix = mixing_matrix.rename(columns={c: f\"signal__{c}\" for c in accepted_columns})\n",
    "\n",
    "# Add dummyvols back in to beginning of matrix\n",
    "mixing_matrix_data = mixing_matrix.to_numpy()\n",
    "first_row = mixing_matrix_data[0, :]\n",
    "leading_rows = np.ones((n_dummy_vols, mixing_matrix.shape[1])) * first_row\n",
    "new_mixing_matrix_arr = np.vstack((leading_rows, mixing_matrix_data))\n",
    "new_mixing_matrix = pd.DataFrame(new_mixing_matrix_arr, columns=mixing_matrix.columns)\n",
    "\n",
    "# Write out to custom confounds folder\n",
    "custom_confounds_folder = os.path.join(deriv_dir, \"custom_confounds_for_xcpd\")\n",
    "os.makedirs(custom_confounds_folder, exist_ok=True)\n",
    "# use the same name as the fMRIPrep confounds, but in the new folder\n",
    "custom_confounds_file = os.path.join(custom_confounds_folder, os.path.basename(confounds_file))\n",
    "new_mixing_matrix.to_csv(custom_confounds_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run XCPD with tedana-derived custom confounds\n",
    "\n",
    "Note: dummy-scans must match between tedana and xcpd\n",
    "```bash\n",
    "docker run --rm -u $(id -u) \\\n",
    "    -v /Users/taylor/Documents/datasets/ds003643:/bids-input:rw \\\n",
    "    -v /Users/taylor/Documents/tsalo/xcp_d/xcp_d:/usr/local/miniconda/lib/python3.8/site-packages/xcp_d \\\n",
    "    -v /Users/taylor/Documents/tsalo/xcp_d_testing/data/license.txt:/license.txt --env FS_LICENSE=/license.txt \\\n",
    "    pennlinc/xcp_d:unstable \\\n",
    "    /bids-input/derivatives/fmriprep \\\n",
    "    /bids-input/derivatives \\\n",
    "    participant \\\n",
    "    -w /bids-input/derivatives/work \\\n",
    "    --participant_label EN100 \\\n",
    "    --nuisance-regressors 27P \\\n",
    "    --custom_confounds /bids-input/derivatives/custom_confounds_for_xcpd \\\n",
    "    --dummy-scans auto \\\n",
    "    --bids-filter-file /bids-input/derivatives/code/filter_file.json \\\n",
    "    -vvv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
