{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.annotate.text:Retaining 14369/14371 studies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14369, 207)\n"
     ]
    }
   ],
   "source": [
    "import nimare as nim\n",
    "from nimare import annotate\n",
    "\n",
    "dset = nim.dataset.Dataset.load('/Users/tsalo/Desktop/ns-dataset/neurosynth_w_cogat.pkl.gz')\n",
    "texts_df = dset.texts\n",
    "texts_df = texts_df.reset_index(drop=False)\n",
    "\n",
    "counts_df = annotate.text.generate_counts(\n",
    "    texts_df, text_column='abstract', tfidf=False, max_df=0.1, min_df=0.05)\n",
    "coordinates_df = dset.coordinates\n",
    "print(counts_df.shape)\n",
    "coordinates_df = coordinates_df.loc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.annotate.gclda:Constructing/Initializing GCLDA Model\n",
      "INFO:nimare.annotate.gclda:IDs mismatch detected: retaining 140 of 14370 unique IDs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Profile printout saved to text file gclda_init. \n"
     ]
    }
   ],
   "source": [
    "%mprun -T gclda_init -f annotate.gclda.GCLDAModel.__init__ model = annotate.gclda.GCLDAModel(counts_df, coordinates_df, mask=dset.masker.mask_img, n_topics=50, symmetric=True, n_regions=2, seed_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/tsalo/Documents/tsalo/NiMARE/nimare/annotate/gclda.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    92   1246.2 MiB   1246.2 MiB       def __init__(\n",
      "    93                                     self,\n",
      "    94                                     count_df,\n",
      "    95                                     coordinates_df,\n",
      "    96                                     mask=\"mni152_2mm\",\n",
      "    97                                     n_topics=100,\n",
      "    98                                     n_regions=2,\n",
      "    99                                     symmetric=True,\n",
      "   100                                     alpha=0.1,\n",
      "   101                                     beta=0.01,\n",
      "   102                                     gamma=0.01,\n",
      "   103                                     delta=1.0,\n",
      "   104                                     dobs=25,\n",
      "   105                                     roi_size=50.0,\n",
      "   106                                     seed_init=1,\n",
      "   107                                 ):\n",
      "   108   1246.2 MiB      0.0 MiB           LGR.info(\"Constructing/Initializing GCLDA Model\")\n",
      "   109   1246.2 MiB      0.0 MiB           count_df = count_df.copy()\n",
      "   110   1246.4 MiB      0.2 MiB           coordinates_df = coordinates_df.copy()\n",
      "   111                             \n",
      "   112                                     # Check IDs from DataFrames\n",
      "   113   1247.1 MiB      0.7 MiB           count_df.index = count_df.index.astype(str)\n",
      "   114   1247.2 MiB      0.1 MiB           count_df[\"id\"] = count_df.index\n",
      "   115   1247.3 MiB      0.1 MiB           count_ids = count_df.index.tolist()\n",
      "   116   1247.3 MiB      0.0 MiB           if \"id\" not in coordinates_df.columns:\n",
      "   117                                         coordinates_df[\"id\"] = coordinates_df.index\n",
      "   118   1247.3 MiB      0.0 MiB           coordinates_df[\"id\"] = coordinates_df[\"id\"].astype(str)\n",
      "   119   1247.3 MiB      0.0 MiB           coord_ids = sorted(list(set(coordinates_df[\"id\"].tolist())))\n",
      "   120   1247.5 MiB      0.1 MiB           ids = sorted(list(set(count_ids).intersection(coord_ids)))\n",
      "   121   1247.5 MiB      0.0 MiB           if len(count_ids) != len(coord_ids) != len(ids):\n",
      "   122   1247.6 MiB      0.1 MiB               union_ids = sorted(list(set(count_ids + coord_ids)))\n",
      "   123   1247.6 MiB      0.0 MiB               LGR.info(\n",
      "   124   1247.6 MiB      0.0 MiB                   \"IDs mismatch detected: retaining {0} of {1} unique \"\n",
      "   125   1247.6 MiB      0.0 MiB                   \"IDs\".format(len(ids), len(union_ids))\n",
      "   126                                         )\n",
      "   127                             \n",
      "   128                                     # Reduce inputs based on shared IDs\n",
      "   129   1247.1 MiB      0.0 MiB           count_df = count_df.loc[count_df[\"id\"].isin(ids)]\n",
      "   130   1247.2 MiB      0.1 MiB           coordinates_df = coordinates_df.loc[coordinates_df[\"id\"].isin(ids)]\n",
      "   131                             \n",
      "   132                                     # --- Checking to make sure parameters are valid\n",
      "   133   1247.2 MiB      0.0 MiB           if (symmetric is True) and (n_regions != 2):\n",
      "   134                                         # symmetric model only valid if R = 2\n",
      "   135                                         raise ValueError(\n",
      "   136                                             \"Cannot run a symmetric model unless # subregions \" \"(n_regions) == 2 !\"\n",
      "   137                                         )\n",
      "   138                             \n",
      "   139                                     # Initialize sampling parameters\n",
      "   140   1247.2 MiB      0.0 MiB           self.iter = 0  # Tracks the global sampling iteration of the model\n",
      "   141   1247.2 MiB      0.0 MiB           self.seed = 0  # Tracks current random seed to use (gets incremented\n",
      "   142                                     # after initialization and each sampling update)\n",
      "   143                             \n",
      "   144                                     # Set up model hyperparameters\n",
      "   145                                     # Pseudo-count hyperparams need to be floats so that when sampling\n",
      "   146                                     # distributions are computed the count matrices/vectors are converted\n",
      "   147                                     # to floats\n",
      "   148                                     self.params = {\n",
      "   149   1247.2 MiB      0.0 MiB               \"n_topics\": n_topics,  # Number of topics (T)\n",
      "   150   1247.2 MiB      0.0 MiB               \"n_regions\": n_regions,  # Number of subregions (R)\n",
      "   151   1247.2 MiB      0.0 MiB               \"alpha\": alpha,  # Prior count on topics for each doc\n",
      "   152   1247.2 MiB      0.0 MiB               \"beta\": beta,  # Prior count on word-types for each topic\n",
      "   153   1247.2 MiB      0.0 MiB               \"gamma\": gamma,  # Prior count added to y-counts when sampling z assignments\n",
      "   154   1247.2 MiB      0.0 MiB               \"delta\": delta,  # Prior count on subregions for each topic\n",
      "   155   1247.2 MiB      0.0 MiB               \"roi_size\": roi_size,  # Default ROI (default covariance spatial\n",
      "   156                                         # region we regularize towards) (not in paper)\n",
      "   157   1247.2 MiB      0.0 MiB               \"dobs\": dobs,  # Sample constant (# observations weighting\n",
      "   158                                         # sigma in direction of default covariance)\n",
      "   159                                         # (not in paper)\n",
      "   160   1247.2 MiB      0.0 MiB               \"symmetric\": symmetric,  # Use constrained symmetry on subregions?\n",
      "   161                                         # (only for n_regions = 2)\n",
      "   162   1247.2 MiB      0.0 MiB               \"seed_init\": seed_init,  # Random seed for initializing model\n",
      "   163                                     }\n",
      "   164                             \n",
      "   165                                     # Add dictionaries for other model info\n",
      "   166   1247.2 MiB      0.0 MiB           self.data = {}\n",
      "   167   1247.2 MiB      0.0 MiB           self.topics = {}\n",
      "   168                             \n",
      "   169                                     # Prepare data\n",
      "   170   1247.2 MiB      0.0 MiB           if isinstance(mask, str) and not op.isfile(mask):\n",
      "   171                                         self.mask = get_template(mask, mask=\"brain\")\n",
      "   172   1247.2 MiB      0.0 MiB           elif isinstance(mask, str) and op.isfile(mask):\n",
      "   173                                         self.mask = nib.load(mask)\n",
      "   174   1247.2 MiB      0.0 MiB           elif isinstance(mask, nib.Nifti1Image):\n",
      "   175   1247.2 MiB      0.0 MiB               self.mask = mask\n",
      "   176                                     else:\n",
      "   177                                         raise Exception('Input \"mask\" could not be figured out.')\n",
      "   178                             \n",
      "   179                                     # Extract document and word indices from count_df\n",
      "   180   1247.2 MiB      0.0 MiB           docidx_mapper = {id_: i for (i, id_) in enumerate(ids)}\n",
      "   181   1247.2 MiB      0.0 MiB           self.ids = ids\n",
      "   182                             \n",
      "   183                                     # Create docidx column\n",
      "   184   1247.3 MiB      0.0 MiB           count_df[\"docidx\"] = count_df[\"id\"].map(docidx_mapper)\n",
      "   185   1247.3 MiB      0.0 MiB           count_df = count_df.dropna(subset=[\"docidx\"])\n",
      "   186   1247.3 MiB      0.0 MiB           count_df = count_df.drop(\"id\", 1)\n",
      "   187                             \n",
      "   188                                     # Remove words not found anywhere in the corpus\n",
      "   189   1247.4 MiB      0.1 MiB           count_df = count_df.loc[:, (count_df != 0).any(axis=0)]\n",
      "   190                             \n",
      "   191                                     # Get updated vocabulary\n",
      "   192                                     # List of word-strings (wtoken_word_idx values are indices into this list)\n",
      "   193   1247.4 MiB      0.0 MiB           vocabulary = count_df.columns.tolist()\n",
      "   194   1247.4 MiB      0.0 MiB           vocabulary.remove(\"docidx\")\n",
      "   195   1247.4 MiB      0.0 MiB           self.vocabulary = vocabulary\n",
      "   196   1247.4 MiB      0.0 MiB           widx_mapper = {word: i for (i, word) in enumerate(self.vocabulary)}\n",
      "   197                             \n",
      "   198                                     # Melt dataframe and create widx column\n",
      "   199   1247.4 MiB      0.0 MiB           widx_df = pd.melt(count_df, id_vars=[\"docidx\"], var_name=\"word\", value_name=\"count\")\n",
      "   200   1247.4 MiB      0.0 MiB           widx_df[\"widx\"] = widx_df[\"word\"].map(widx_mapper)\n",
      "   201                             \n",
      "   202                                     # Replicate rows based on count\n",
      "   203   1248.6 MiB      1.1 MiB           widx_df = widx_df.loc[np.repeat(widx_df.index.values, widx_df[\"count\"])]\n",
      "   204   1208.6 MiB      0.0 MiB           widx_df = widx_df[[\"docidx\", \"widx\"]].astype(int)\n",
      "   205   1208.9 MiB      0.2 MiB           widx_df.sort_values(by=[\"docidx\", \"widx\"], inplace=True)\n",
      "   206                             \n",
      "   207                                     # List of document-indices for word-tokens\n",
      "   208   1208.9 MiB      0.0 MiB           self.data[\"wtoken_doc_idx\"] = widx_df[\"docidx\"].tolist()\n",
      "   209                                     # List of word-indices for word-tokens\n",
      "   210   1208.9 MiB      0.0 MiB           self.data[\"wtoken_word_idx\"] = widx_df[\"widx\"].tolist()\n",
      "   211                             \n",
      "   212                                     # Import all peak-indices into lists\n",
      "   213   1208.9 MiB      0.0 MiB           coordinates_df[\"docidx\"] = coordinates_df[\"id\"].astype(str).map(docidx_mapper)\n",
      "   214   1208.9 MiB      0.0 MiB           coordinates_df = coordinates_df.dropna(subset=[\"docidx\"])\n",
      "   215   1168.9 MiB      0.0 MiB           coordinates_df = coordinates_df[[\"docidx\", \"x\", \"y\", \"z\"]]\n",
      "   216   1168.9 MiB      0.0 MiB           coordinates_df[\"docidx\"] = coordinates_df[\"docidx\"].astype(int)\n",
      "   217                                     # List of document-indices for peak-tokens x\n",
      "   218   1168.9 MiB      0.0 MiB           self.data[\"ptoken_doc_idx\"] = coordinates_df[\"docidx\"].tolist()\n",
      "   219   1168.9 MiB      0.0 MiB           self.data[\"peak_vals\"] = coordinates_df[[\"x\", \"y\", \"z\"]].values\n",
      "   220                             \n",
      "   221                                     # Seed random number generator\n",
      "   222   1168.9 MiB      0.0 MiB           np.random.seed(self.params[\"seed_init\"])  # pylint: disable=no-member\n",
      "   223                             \n",
      "   224                                     # Preallocate vectors of assignment indices\n",
      "   225   1168.9 MiB      0.0 MiB           self.topics[\"wtoken_topic_idx\"] = np.zeros(\n",
      "   226   1168.9 MiB      0.0 MiB               len(self.data[\"wtoken_word_idx\"]), dtype=int\n",
      "   227                                     )  # word->topic assignments\n",
      "   228                             \n",
      "   229                                     # Randomly initialize peak->topic assignments (y) ~ unif(1...n_topics)\n",
      "   230   1168.9 MiB      0.0 MiB           self.topics[\"peak_topic_idx\"] = np.random.randint(\n",
      "   231   1168.9 MiB      0.0 MiB               self.params[\"n_topics\"],  # pylint: disable=no-member\n",
      "   232   1168.9 MiB      0.0 MiB               size=(len(self.data[\"ptoken_doc_idx\"])),\n",
      "   233                                     )\n",
      "   234                             \n",
      "   235   1168.9 MiB      0.0 MiB           self.topics[\"peak_region_idx\"] = np.zeros(\n",
      "   236   1168.9 MiB      0.0 MiB               len(self.data[\"ptoken_doc_idx\"]), dtype=int\n",
      "   237                                     )  # peak->region assignments\n",
      "   238                             \n",
      "   239                                     # Preallocate count matrices\n",
      "   240                                     # Peaks: D x T: Number of peak-tokens assigned to each topic per document\n",
      "   241   1168.9 MiB      0.0 MiB           self.topics[\"n_peak_tokens_doc_by_topic\"] = np.zeros(\n",
      "   242   1168.9 MiB      0.0 MiB               (len(self.ids), self.params[\"n_topics\"]), dtype=int\n",
      "   243                                     )\n",
      "   244                             \n",
      "   245                                     # Peaks: R x T: Number of peak-tokens assigned to each subregion per topic\n",
      "   246   1168.9 MiB      0.0 MiB           self.topics[\"n_peak_tokens_region_by_topic\"] = np.zeros(\n",
      "   247   1168.9 MiB      0.0 MiB               (self.params[\"n_regions\"], self.params[\"n_topics\"]), dtype=int\n",
      "   248                                     )\n",
      "   249                             \n",
      "   250                                     # Words: W x T: Number of word-tokens assigned to each topic per word-type\n",
      "   251   1168.9 MiB      0.0 MiB           self.topics[\"n_word_tokens_word_by_topic\"] = np.zeros(\n",
      "   252   1168.9 MiB      0.0 MiB               (len(self.vocabulary), self.params[\"n_topics\"]), dtype=int\n",
      "   253                                     )\n",
      "   254                             \n",
      "   255                                     # Words: D x T: Number of word-tokens assigned to each topic per document\n",
      "   256   1168.9 MiB      0.0 MiB           self.topics[\"n_word_tokens_doc_by_topic\"] = np.zeros(\n",
      "   257   1168.9 MiB      0.0 MiB               (len(self.ids), self.params[\"n_topics\"]), dtype=int\n",
      "   258                                     )\n",
      "   259                             \n",
      "   260                                     # Words: 1 x T: Total number of word-tokens assigned to each topic (across all docs)\n",
      "   261   1168.9 MiB      0.0 MiB           self.topics[\"total_n_word_tokens_by_topic\"] = np.zeros(\n",
      "   262   1168.9 MiB      0.0 MiB               (1, self.params[\"n_topics\"]), dtype=int\n",
      "   263                                     )\n",
      "   264                             \n",
      "   265                                     # Preallocate Gaussians for all subregions\n",
      "   266                                     # Regions_Mu & Regions_Sigma: Gaussian mean and covariance for all\n",
      "   267                                     # subregions of all topics\n",
      "   268                                     # Formed using lists (over topics) of lists (over subregions) of numpy\n",
      "   269                                     # arrays\n",
      "   270                                     #   regions_mu = (n_topics, n_regions, 1, n_peak_dims)\n",
      "   271                                     #   regions_sigma = (n_topics, n_regions, n_peak_dims, n_peak_dims)\n",
      "   272                                     # (\\mu^{(t)}_r)\n",
      "   273   1168.9 MiB      0.0 MiB           self.topics[\"regions_mu\"] = np.zeros(\n",
      "   274   1168.9 MiB      0.0 MiB               (self.params[\"n_topics\"], self.params[\"n_regions\"], 1, self.data[\"peak_vals\"].shape[1])\n",
      "   275                                     )\n",
      "   276                                     # (\\sigma^{(t)}_r)\n",
      "   277   1168.9 MiB      0.0 MiB           self.topics[\"regions_sigma\"] = np.zeros(\n",
      "   278                                         (\n",
      "   279   1168.9 MiB      0.0 MiB                   self.params[\"n_topics\"],\n",
      "   280   1168.9 MiB      0.0 MiB                   self.params[\"n_regions\"],\n",
      "   281   1168.9 MiB      0.0 MiB                   self.data[\"peak_vals\"].shape[1],\n",
      "   282   1168.9 MiB      0.0 MiB                   self.data[\"peak_vals\"].shape[1],\n",
      "   283                                         )\n",
      "   284                                     )\n",
      "   285                             \n",
      "   286                                     # Initialize lists for tracking log-likelihood of data over sampling iterations\n",
      "   287   1168.9 MiB      0.0 MiB           self.loglikely_iter = []  # Tracks iteration we compute each loglikelihood at\n",
      "   288   1168.9 MiB      0.0 MiB           self.loglikely_x = []  # Tracks log-likelihood of peak tokens\n",
      "   289   1168.9 MiB      0.0 MiB           self.loglikely_w = []  # Tracks log-likelihood of word tokens\n",
      "   290   1168.9 MiB      0.0 MiB           self.loglikely_tot = []  # Tracks log-likelihood of peak + word tokens\n",
      "   291                             \n",
      "   292                                     # Initialize peak->subregion assignments (r)\n",
      "   293   1168.9 MiB      0.0 MiB           if not self.params[\"symmetric\"]:\n",
      "   294                                         # if symmetric model use deterministic assignment :\n",
      "   295                                         #     if peak_val[0] > 0, r = 1, else r = 0\n",
      "   296                                         self.topics[\"peak_region_idx\"][:] = np.random.randint(\n",
      "   297                                             self.params[\"n_regions\"],  # pylint: disable=no-member\n",
      "   298                                             size=(len(self.data[\"ptoken_doc_idx\"])),\n",
      "   299                                         )\n",
      "   300                                     else:\n",
      "   301                                         # if asymmetric model, randomly sample r ~ unif(1...n_regions)\n",
      "   302   1169.0 MiB      0.0 MiB               self.topics[\"peak_region_idx\"][:] = (self.data[\"peak_vals\"][:, 0] > 0).astype(int)\n",
      "   303                             \n",
      "   304                                     # Update model vectors and count matrices to reflect y and r assignments\n",
      "   305   1169.0 MiB      0.0 MiB           for i_ptoken in range(len(self.data[\"ptoken_doc_idx\"])):\n",
      "   306                                         # document -idx (d)\n",
      "   307   1169.0 MiB      0.0 MiB               doc = self.data[\"ptoken_doc_idx\"][i_ptoken]\n",
      "   308   1169.0 MiB      0.0 MiB               topic = self.topics[\"peak_topic_idx\"][i_ptoken]  # peak-token -> topic assignment (y_i)\n",
      "   309   1169.0 MiB      0.0 MiB               region = self.topics[\"peak_region_idx\"][\n",
      "   310   1169.0 MiB      0.0 MiB                   i_ptoken\n",
      "   311                                         ]  # peak-token -> subregion assignment (c_i)\n",
      "   312   1169.0 MiB      0.0 MiB               self.topics[\"n_peak_tokens_doc_by_topic\"][\n",
      "   313   1169.0 MiB      0.0 MiB                   doc, topic\n",
      "   314   1169.0 MiB      0.0 MiB               ] += 1  # Increment document-by-topic counts\n",
      "   315   1169.0 MiB      0.0 MiB               self.topics[\"n_peak_tokens_region_by_topic\"][\n",
      "   316   1169.0 MiB      0.0 MiB                   region, topic\n",
      "   317   1169.0 MiB      0.0 MiB               ] += 1  # Increment region-by-topic\n",
      "   318                             \n",
      "   319                                     # Randomly Initialize Word->Topic Assignments (z) for each word\n",
      "   320                                     # token w_i: sample z_i proportional to p(topic|doc_i)\n",
      "   321   1169.1 MiB      0.0 MiB           for i_wtoken in range(len(self.data[\"wtoken_word_idx\"])):\n",
      "   322                                         # w_i word-type\n",
      "   323   1169.1 MiB      0.0 MiB               word = self.data[\"wtoken_word_idx\"][i_wtoken]\n",
      "   324                             \n",
      "   325                                         # w_i doc-index\n",
      "   326   1169.1 MiB      0.0 MiB               doc = self.data[\"wtoken_doc_idx\"][i_wtoken]\n",
      "   327                             \n",
      "   328                                         # Estimate p(t|d) for current doc\n",
      "   329   1169.1 MiB      0.0 MiB               p_topic_g_doc = self.topics[\"n_peak_tokens_doc_by_topic\"][doc] + self.params[\"gamma\"]\n",
      "   330                             \n",
      "   331                                         # Sample a topic from p(t|d) for the z-assignment\n",
      "   332                                         # Compute a cdf of the sampling distribution for z\n",
      "   333   1169.1 MiB      0.0 MiB               probs = np.cumsum(p_topic_g_doc)\n",
      "   334                                         # Which elements of cdf are less than random sample?\n",
      "   335   1169.1 MiB      0.0 MiB               sample_locs = np.where(probs < np.random.rand() * probs[-1])[\n",
      "   336   1169.1 MiB      0.0 MiB                   0\n",
      "   337                                         ]  # pylint: disable=no-member\n",
      "   338                                         # How many elements of cdf are less than sample\n",
      "   339                                         # z = # elements of cdf less than rand-sample\n",
      "   340   1169.1 MiB      0.0 MiB               topic = len(sample_locs)\n",
      "   341                             \n",
      "   342                                         # Update model assignment vectors and count-matrices to reflect z\n",
      "   343                                         self.topics[\"wtoken_topic_idx\"][\n",
      "   344                                             i_wtoken\n",
      "   345   1169.1 MiB      0.0 MiB               ] = topic  # Word-token -> topic assignment (z_i)\n",
      "   346   1169.1 MiB      0.0 MiB               self.topics[\"n_word_tokens_word_by_topic\"][word, topic] += 1\n",
      "   347   1169.1 MiB      0.0 MiB               self.topics[\"total_n_word_tokens_by_topic\"][0, topic] += 1\n",
      "   348   1169.1 MiB      0.0 MiB               self.topics[\"n_word_tokens_doc_by_topic\"][doc, topic] += 1\n"
     ]
    }
   ],
   "source": [
    "print(open('gclda_init', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.annotate.gclda:Iter 0001: Sampling z\n",
      "INFO:nimare.annotate.gclda:Iter 0001: Sampling y|r\n",
      "INFO:nimare.annotate.gclda:Iter 0001: Updating spatial params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Profile printout saved to text file gclda. \n"
     ]
    }
   ],
   "source": [
    "%mprun -T gclda -f annotate.gclda.GCLDAModel.fit model.fit(n_iters=1, loglikely_freq=1000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/tsalo/Documents/tsalo/NiMARE/nimare/annotate/gclda.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "   350   1168.7 MiB   1168.7 MiB       def fit(self, n_iters=10000, loglikely_freq=10, verbose=1):\n",
      "   351                                     \"\"\"\n",
      "   352                                     Run multiple iterations.\n",
      "   353                             \n",
      "   354                                     Parameters\n",
      "   355                                     ----------\n",
      "   356                                     n_iters : :obj:`int`, optional\n",
      "   357                                         Number of iterations to run. Default is 10000.\n",
      "   358                                     loglikely_freq : :obj:`int`, optional\n",
      "   359                                         The frequency with which log-likelihood is updated. Default value\n",
      "   360                                         is 1 (log-likelihood is updated every iteration).\n",
      "   361                                     verbose : {0, 1, 2}, optional\n",
      "   362                                         Determines how much info is printed to console. 0 = none,\n",
      "   363                                         1 = a little, 2 = a lot. Default value is 2.\n",
      "   364                                     \"\"\"\n",
      "   365   1168.7 MiB      0.0 MiB           if self.iter == 0:\n",
      "   366                                         # Get Initial Spatial Parameter Estimates\n",
      "   367   1168.8 MiB      0.1 MiB               self._update_regions()\n",
      "   368                             \n",
      "   369                                         # Get Log-Likelihood of data for Initialized model and save to\n",
      "   370                                         # variables tracking loglikely\n",
      "   371   1170.8 MiB      2.0 MiB               self.compute_log_likelihood()\n",
      "   372                             \n",
      "   373   1170.9 MiB      0.0 MiB           for i in range(self.iter, n_iters):\n",
      "   374   1170.8 MiB      0.0 MiB               self.iter += 1  # Update total iteration count\n",
      "   375                             \n",
      "   376   1170.8 MiB      0.0 MiB               if verbose == 2:\n",
      "   377   1170.8 MiB      0.0 MiB                   LGR.info(\"Iter {0:04d}: Sampling z\".format(self.iter))\n",
      "   378   1170.8 MiB      0.0 MiB               self.seed += 1\n",
      "   379   1170.9 MiB      0.0 MiB               self._update_word_topic_assignments(self.seed)  # Update z-assignments\n",
      "   380                             \n",
      "   381   1170.9 MiB      0.0 MiB               if verbose == 2:\n",
      "   382   1170.9 MiB      0.0 MiB                   LGR.info(\"Iter {0:04d}: Sampling y|r\".format(self.iter))\n",
      "   383   1170.9 MiB      0.0 MiB               self.seed += 1\n",
      "   384   1170.9 MiB      0.0 MiB               self._update_peak_assignments(self.seed)  # Update y-assignments\n",
      "   385                             \n",
      "   386   1170.9 MiB      0.0 MiB               if verbose == 2:\n",
      "   387   1170.9 MiB      0.0 MiB                   LGR.info(\"Iter {0:04d}: Updating spatial params\".format(self.iter))\n",
      "   388   1170.9 MiB      0.0 MiB               self._update_regions()  # Update gaussian estimates for all subregions\n",
      "   389                             \n",
      "   390                                         # Only update loglikelihood every 'loglikely_freq' iterations\n",
      "   391                                         # (Computing log-likelihood isn't necessary and slows things down a bit)\n",
      "   392   1170.9 MiB      0.0 MiB               if self.iter % loglikely_freq == 0:\n",
      "   393                                             if verbose == 2:\n",
      "   394                                                 LGR.info(\"Iter {0:04d}: Computing log-likelihood\".format(self.iter))\n",
      "   395                                             # Compute log-likelihood of model in current state\n",
      "   396                                             self.compute_log_likelihood()\n",
      "   397                                             if verbose > 0:\n",
      "   398                                                 LGR.info(\n",
      "   399                                                     \"Iter {0:04d} Log-likely: x = {1:10.1f}, w = {2:10.1f}, \"\n",
      "   400                                                     \"tot = {3:10.1f}\".format(\n",
      "   401                                                         self.iter,\n",
      "   402                                                         self.loglikely_x[-1],\n",
      "   403                                                         self.loglikely_w[-1],\n",
      "   404                                                         self.loglikely_tot[-1],\n",
      "   405                                                     )\n",
      "   406                                                 )\n",
      "   407                             \n",
      "   408                                     # TODO: Handle this more elegantly\n",
      "   409                                     (\n",
      "   410                                         self.p_topic_g_voxel,\n",
      "   411                                         self.p_voxel_g_topic,\n",
      "   412                                         self.p_topic_g_word,\n",
      "   413                                         self.p_word_g_topic,\n",
      "   414   1520.9 MiB    350.0 MiB           ) = self.get_probs()\n"
     ]
    }
   ],
   "source": [
    "print(open('gclda', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.annotate.gclda:Iter 0002: Sampling z\n",
      "INFO:nimare.annotate.gclda:Iter 0002: Sampling y|r\n",
      "INFO:nimare.annotate.gclda:Iter 0002: Updating spatial params\n",
      "INFO:nimare.annotate.gclda:Iter 0003: Sampling z\n",
      "INFO:nimare.annotate.gclda:Iter 0003: Sampling y|r\n",
      "INFO:nimare.annotate.gclda:Iter 0003: Updating spatial params\n",
      "INFO:nimare.annotate.gclda:Iter 0004: Sampling z\n",
      "INFO:nimare.annotate.gclda:Iter 0004: Sampling y|r\n",
      "INFO:nimare.annotate.gclda:Iter 0004: Updating spatial params\n",
      "INFO:nimare.annotate.gclda:Iter 0005: Sampling z\n",
      "INFO:nimare.annotate.gclda:Iter 0005: Sampling y|r\n",
      "INFO:nimare.annotate.gclda:Iter 0005: Updating spatial params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'gclda_time'. \n"
     ]
    }
   ],
   "source": [
    "%lprun -T gclda_time -f annotate.gclda.GCLDAModel._update_peak_assignments model.fit(n_iters=5, loglikely_freq=1000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 3.54598 s\n",
      "File: /Users/tsalo/Documents/tsalo/NiMARE/nimare/annotate/gclda.py\n",
      "Function: _update_peak_assignments at line 467\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   467                                               def _update_peak_assignments(self, randseed):\n",
      "   468                                                   \"\"\"\n",
      "   469                                                   Update y / r indicator variables assigning peaks->topics/subregions.\n",
      "   470                                           \n",
      "   471                                                   Parameters\n",
      "   472                                                   ----------\n",
      "   473                                                   randseed : :obj:`int`\n",
      "   474                                                       Random seed for this iteration.\n",
      "   475                                                   \"\"\"\n",
      "   476                                                   # Seed random number generator\n",
      "   477         4        102.0     25.5      0.0          np.random.seed(randseed)  # pylint: disable=no-member\n",
      "   478                                           \n",
      "   479                                                   # Retrieve p(x|r,y) for all subregions\n",
      "   480         4     338317.0  84579.2      9.5          peak_probs = self._get_peak_probs(self)\n",
      "   481                                           \n",
      "   482                                                   # Iterate over all peaks x, and sample a new y and r assignment for each\n",
      "   483     19984      16325.0      0.8      0.5          for i_ptoken in range(len(self.data[\"ptoken_doc_idx\"])):\n",
      "   484     19980      17380.0      0.9      0.5              doc = self.data[\"ptoken_doc_idx\"][i_ptoken]\n",
      "   485     19980      16914.0      0.8      0.5              topic = self.topics[\"peak_topic_idx\"][i_ptoken]\n",
      "   486     19980      17684.0      0.9      0.5              region = self.topics[\"peak_region_idx\"][i_ptoken]\n",
      "   487                                           \n",
      "   488                                                       # Decrement count in Subregion x Topic count matrix\n",
      "   489     19980      34681.0      1.7      1.0              self.topics[\"n_peak_tokens_region_by_topic\"][region, topic] -= 1\n",
      "   490                                           \n",
      "   491                                                       # Decrement count in Document x Topic count matrix\n",
      "   492     19980      28657.0      1.4      0.8              self.topics[\"n_peak_tokens_doc_by_topic\"][doc, topic] -= 1\n",
      "   493                                           \n",
      "   494                                                       # Retrieve the probability of generating current x from all\n",
      "   495                                                       # subregions: [R x T] array of probs\n",
      "   496     19980      42553.0      2.1      1.2              p_x_subregions = (peak_probs[i_ptoken, :, :]).transpose()\n",
      "   497                                           \n",
      "   498                                                       # Compute the probabilities of all subregions given doc\n",
      "   499                                                       #     p(r|d) ~ p(r|t) * p(t|d)\n",
      "   500                                                       # Counts of subregions per topic + prior: p(r|t)\n",
      "   501     19980     130803.0      6.5      3.7              p_region_g_topic = self.topics[\"n_peak_tokens_region_by_topic\"] + self.params[\"delta\"]\n",
      "   502                                           \n",
      "   503                                                       # Normalize the columns such that each topic's distribution over\n",
      "   504                                                       # subregions sums to 1\n",
      "   505     19980     368221.0     18.4     10.4              p_region_g_topic = p_region_g_topic / np.sum(p_region_g_topic, axis=0)\n",
      "   506                                           \n",
      "   507                                                       # Counts of topics per document + prior: p(t|d)\n",
      "   508                                                       p_topic_g_doc = (\n",
      "   509     19980      97860.0      4.9      2.8                  self.topics[\"n_peak_tokens_doc_by_topic\"][doc, :] + self.params[\"alpha\"]\n",
      "   510                                                       )\n",
      "   511                                           \n",
      "   512                                                       # Reshape from (ntopics,) to (nregions, ntopics) with duplicated rows\n",
      "   513     19980     107015.0      5.4      3.0              p_topic_g_doc = np.array([p_topic_g_doc] * self.params[\"n_regions\"])\n",
      "   514                                           \n",
      "   515                                                       # Compute p(subregion | document): p(r|d) ~ p(r|t) * p(t|d)\n",
      "   516                                                       # [R x T] array of probs\n",
      "   517     19980      42813.0      2.1      1.2              p_region_g_doc = p_topic_g_doc * p_region_g_topic\n",
      "   518                                           \n",
      "   519                                                       # Compute the multinomial probability: p(z|y)\n",
      "   520                                                       # Need the current vector of all z and y assignments for current doc\n",
      "   521                                                       # The multinomial from which z is sampled is proportional to number\n",
      "   522                                                       # of y assigned to each topic, plus constant \\gamma\n",
      "   523     19980      68017.0      3.4      1.9              doc_y_counts = self.topics[\"n_peak_tokens_doc_by_topic\"][doc, :] + self.params[\"gamma\"]\n",
      "   524     19980      25262.0      1.3      0.7              doc_z_counts = self.topics[\"n_word_tokens_doc_by_topic\"][doc, :]\n",
      "   525     19980      14591.0      0.7      0.4              p_peak_g_topic = self._compute_prop_multinomial_from_zy_vectors(\n",
      "   526     19980     511062.0     25.6     14.4                  doc_z_counts, doc_y_counts\n",
      "   527                                                       )\n",
      "   528                                           \n",
      "   529                                                       # Reshape from (ntopics,) to (nregions, ntopics) with duplicated rows\n",
      "   530     19980      97322.0      4.9      2.7              p_peak_g_topic = np.array([p_peak_g_topic] * self.params[\"n_regions\"])\n",
      "   531                                           \n",
      "   532                                                       # Get the full sampling distribution:\n",
      "   533                                                       # [R x T] array containing the proportional probability of all y/r combinations\n",
      "   534     19980     118323.0      5.9      3.3              probs_pdf = p_x_subregions * p_region_g_doc * p_peak_g_topic\n",
      "   535                                           \n",
      "   536                                                       # Convert from a [R x T] matrix into a [R*T x 1] array we can sample from\n",
      "   537     19980      70039.0      3.5      2.0              probs_pdf = probs_pdf.transpose().ravel()\n",
      "   538                                           \n",
      "   539                                                       # Normalize the sampling distribution\n",
      "   540     19980     325565.0     16.3      9.2              probs_pdf = probs_pdf / np.sum(probs_pdf)\n",
      "   541                                           \n",
      "   542                                                       # Sample a single element (corresponding to a y_i and c_i assignment\n",
      "   543                                                       # for the peak token) from the sampling distribution\n",
      "   544                                                       # Returns a [1 x R*T] vector with a '1' in location that was sampled\n",
      "   545     19980     653570.0     32.7     18.4              vec = np.random.multinomial(1, probs_pdf)  # pylint: disable=no-member\n",
      "   546     19980     100483.0      5.0      2.8              sample_idx = np.where(vec)[0][0]  # Extract linear index value from vector\n",
      "   547                                           \n",
      "   548                                                       # Transform the linear index of the sampled element into the\n",
      "   549                                                       # subregion/topic (r/y) assignment indices\n",
      "   550                                                       # Subregion sampled (r)\n",
      "   551     19980      14860.0      0.7      0.4              region = np.remainder(\n",
      "   552     19980      89832.0      4.5      2.5                  sample_idx, self.params[\"n_regions\"]\n",
      "   553                                                       )  # pylint: disable=no-member\n",
      "   554     19980      89650.0      4.5      2.5              topic = int(np.floor(sample_idx / self.params[\"n_regions\"]))  # Topic sampled (y)\n",
      "   555                                           \n",
      "   556                                                       # Update the indices and the count matrices using the sampled y/r assignments\n",
      "   557                                                       # Increment count in Subregion x Topic count matrix\n",
      "   558     19980      43607.0      2.2      1.2              self.topics[\"n_peak_tokens_region_by_topic\"][region, topic] += 1\n",
      "   559                                                       # Increment count in Document x Topic count matrix\n",
      "   560     19980      29751.0      1.5      0.8              self.topics[\"n_peak_tokens_doc_by_topic\"][doc, topic] += 1\n",
      "   561     19980      17629.0      0.9      0.5              self.topics[\"peak_topic_idx\"][i_ptoken] = topic  # Update y->topic assignment\n",
      "   562     19980      17088.0      0.9      0.5              self.topics[\"peak_region_idx\"][i_ptoken] = region  # Update y->subregion assignment\n"
     ]
    }
   ],
   "source": [
    "print(open('gclda_time', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>ability</th>\n",
       "      <th>action</th>\n",
       "      <th>active</th>\n",
       "      <th>addition</th>\n",
       "      <th>additional</th>\n",
       "      <th>adults</th>\n",
       "      <th>affective</th>\n",
       "      <th>...</th>\n",
       "      <th>volume</th>\n",
       "      <th>volunteers</th>\n",
       "      <th>voxel</th>\n",
       "      <th>vs</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>working</th>\n",
       "      <th>working memory</th>\n",
       "      <th>years</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9065511-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084599-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9114263-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9185551-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9256495-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29779744-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29782510-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29787789-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29845005-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29845006-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14369 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            10  12  20  ability  action  active  addition  additional  adults  \\\n",
       "id                                                                              \n",
       "9065511-1    0   0   0        0       0       0         0           0       0   \n",
       "9084599-1    0   0   0        0       0       0         0           0       0   \n",
       "9114263-1    0   0   0        0       0       0         1           0       0   \n",
       "9185551-1    0   0   0        0       0       0         0           0       0   \n",
       "9256495-1    0   0   0        0       0       2         0           0       0   \n",
       "...         ..  ..  ..      ...     ...     ...       ...         ...     ...   \n",
       "29779744-1   0   0   0        0       0       0         0           0       0   \n",
       "29782510-1   0   0   0        0       0       0         0           0       0   \n",
       "29787789-1   0   0   0        0       0       1         0           0       0   \n",
       "29845005-1   0   0   0        0       0       0         0           0       0   \n",
       "29845006-1   0   0   0        0       0       0         0           0       0   \n",
       "\n",
       "            affective  ...  volume  volunteers  voxel  vs  word  words  \\\n",
       "id                     ...                                               \n",
       "9065511-1           0  ...       0           0      0   0     0      0   \n",
       "9084599-1           0  ...       0           1      0   0     0      0   \n",
       "9114263-1           0  ...       0           0      0   0     0      0   \n",
       "9185551-1           0  ...       0           0      0   0     0      0   \n",
       "9256495-1           0  ...       0           0      0   0     0      0   \n",
       "...               ...  ...     ...         ...    ...  ..   ...    ...   \n",
       "29779744-1          0  ...       0           0      0   0     0      0   \n",
       "29782510-1          0  ...       0           0      0   0     0      0   \n",
       "29787789-1          0  ...       0           0      0   0     0      0   \n",
       "29845005-1          1  ...       0           0      0   0     0      0   \n",
       "29845006-1          0  ...       0           0      0   0     0      0   \n",
       "\n",
       "            working  working memory  years  young  \n",
       "id                                                 \n",
       "9065511-1         0               0      0      0  \n",
       "9084599-1         0               0      0      0  \n",
       "9114263-1         0               0      0      0  \n",
       "9185551-1         0               0      0      0  \n",
       "9256495-1         0               0      0      0  \n",
       "...             ...             ...    ...    ...  \n",
       "29779744-1        0               0      0      0  \n",
       "29782510-1        0               0      0      0  \n",
       "29787789-1        0               0      0      0  \n",
       "29845005-1        0               0      0      0  \n",
       "29845006-1        0               0      0      0  \n",
       "\n",
       "[14369 rows x 207 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
